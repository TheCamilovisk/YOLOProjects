{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Computer Vision is a fields of computer science that focuses on enabling artificial systems to extract information from images and its variants (e.g.: video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, medical scanning devices, etc.). It makes use of algorithmic models that allow a computer to \"teach\" itself the context of visual data, learning the patterns that distinguish an image from another. Computational vision is rapidly gaining popularity for automated AI vision inspection, remote monitoring, and automation.\n",
    "\n",
    "Computer Vision has become the backbone of numerous practical applications that significantly impact our daily lives and companies across industries, from retail to security, healthcare, construction, automotive, manufacturing, logistics, and agriculture.\n",
    "\n",
    "In this scenario, one of the most groundbreaking approaches for Computer Vision is the **You Only Look Once** (YOLO) models family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development History\n",
    "\n",
    "The first version of YOLO(You Look Only Once) was conceived by Joseph Redmon at al. in their 2015 [paper](https://arxiv.org/abs/1506.02640), and represented a groundbreaking approach in Computer Vision, particularly in object *detection tasks.* At the, time the conventional object detection frameworks (e.g.: RCNN) relied on a two-step approach: for a given image, one model is responsible for extraction of regions of objects, and a second model is responsible for classification and refinement of localization of objects. YOLOv1 (how the first version became known) challenged this convention by proposing a single neural network that predictions bounding boxes and class probabilities directly from full images in one evaluation. The approach significantly increased the speed of detection, making real-time object detection feasible.\n",
    "\n",
    "![RCNN pipeline](imgs/rcnn-pipeline.png)\n",
    "*RCNN's multi-stage detection. Credits: RCNN original [paper](https://arxiv.org/abs/1311.2524).*\n",
    "\n",
    "![YOLOv1 pipeline](imgs/yolov1-pipeline.png)\n",
    "*YOLOv1 unified detection. Credits: YOLOv1 original [paper](https://arxiv.org/abs/1506.02640).*\n",
    "\n",
    "Following the initial release, the YOLO architecture underwent several iterations and improvements, leading to version like YOLOv2 ([YOLO9000](https://arxiv.org/abs/1612.08242)), [YOLOv3](https://arxiv.org/abs/1804.02767), and further, each introducing enhancements in speed, accuracy, and the ability to detect smaller objects. [YOLOv4](https://arxiv.org/abs/2004.10934), introduced by Alexey Bochkovskiy, focused on optimizing the speed, accuracy trade-off, making it highly efficient without specialized hardware.\n",
    "\n",
    "The [Ultralytics](https://www.ultralytics.com/) team contributed significantly to the YOLO legacy with their [YOLOv5](https://docs.ultralytics.com/yolov5/) model, which brought improvements in terms of simplicity, speed, and performance. They continued this trend with the development of [YOLOv6](https://arxiv.org/abs/2209.02976) and [YOLOv8](https://docs.ultralytics.com/), which incorporates advanced features and improving upon the accuracy and efficiency of its predecessors.\n",
    "\n",
    "YOLOv8 also supports a full range of vision AI tasks, including detection, segmentation, pose estimation, tracking and classification. This versatility allows users to leverage YOLOv8's capabilities across diverse application and domains.\n",
    "\n",
    "Currently, [YOLO-NAS](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md) and [YOLOv9](https://arxiv.org/abs/2402.13616) were conceived with remarkable improvements in efficiency, accuracy and adaptability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "<img src=\"imgs/yolov8_architecture.jpg\" alt=\"YOLOv8 Architecture\" width=800>*Credits: GitHub user [RangeKing](https://github.com/RangeKing) ([original post](https://github.com/ultralytics/ultralytics/issues/189))*</img>\n",
    "\n",
    "The YOLOv8 architecture was designed to perform object detection tasks with high efficiency and accuracy. While maintaining the core principle of performing object detection in a single pass through the network, YOLOv8 introduces several key improvements and features to enhance performance, incorporating advanced techniques such as:\n",
    "- **Cross-stage Partial Networks (CSPNet):** A backbone designed to reduce redundancy in network layers, reducing it's complexity and improving learning efficiency and model scalability without compromising performance.\n",
    "- **Path Aggregation Network (PANet):** An architecture that enahnces feature extraction and integration ensuring rich semantic information is carried through the network for accurate detection.\n",
    "- **Spatial Pyramid Pooling (SPP):** A pooling strategy that increases the network's robustness to object scale variations, improving detection of objects of various sizes.\n",
    "\n",
    "Additionally, YOLOv8 employs advanced data augmentation techniques and loss functions ([CIoU](https://arxiv.org/abs/1911.08287) and [DFL](https://arxiv.org/abs/2006.04388)) to fine-tune the model's performance further, ensuring it remains robust against a wide variety of images and scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Features\n",
    "\n",
    "YOLOv8 support a versatile range of Computer Vision tasks and pre-trained models for each:\n",
    "- **Image classification**, with models pre-trained in the [ImageNet dataset](https://www.image-net.org/).\n",
    "- **Object detections**, **object segmentation** and **human pose estimation**, with models pre-trained in the [COCO dataset](https://cocodataset.org/#home).\n",
    "\n",
    "Additionally, there are model variatesion for each of these tasks, each variation targeted to run on systems with different hardware specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Requirements\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import cv2 as cv\n",
    "import imageio\n",
    "import numpy as np\n",
    "from pytube import YouTube\n",
    "from ultralytics.models import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Resize Function\n",
    "\n",
    "This function resizes a image to meet a fixed width. This is helpful when creating GIF's, as too big images can result in unwanted big GIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imresize(img: np.ndarray, width: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resizes an image to a specified width while maintaining aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Input image in the form of a NumPy array.\n",
    "        width (int): Desired width of the output image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Resized image as a NumPy array.\n",
    "    \"\"\"\n",
    "    _, old_width, _ = img.shape\n",
    "    factor = width / old_width\n",
    "    return cv.resize(img, None, fx=factor, fy=factor, interpolation=cv.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GIF for Predictions\n",
    "\n",
    "This functions converts a sequence of mode predictions on video data into an animated GIF. It takes as input the path to the video file, the output path for the GIF, the model used for predictions, and optimal parameters for duration and *frames per second* (*pfs*). The function processes the video, applies the model to generate the predictions for each frame, and compiles these frames into a GIF. This utility is particularly useful for demonstrating object detection, segmentation and pose estimation capabilities in a dynamic, easily shareable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif_for_predictions(\n",
    "    video_path: str,\n",
    "    output_gif_path: str,\n",
    "    inference_func: Callable[[np.ndarray], np.ndarray],\n",
    "    img_width: int = 640,\n",
    "    max_frames: int = 300,\n",
    "    fps: int = 15,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a GIF from a video by applying an inference function to each frame.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input video file.\n",
    "        output_gif_path (str): Path to save the output GIF file.\n",
    "        inference_func (Callable[[np.ndarray], np.ndarray]): Function that performs inference on each frame.\n",
    "        img_width (int, optional): Desired width of each frame in the GIF. Defaults to 640.\n",
    "        max_frames (int, optional): Maximum number of frames to process from the video. Defaults to 300.\n",
    "        fps (int, optional): Frames per second for the output GIF. Defaults to 30.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    img_list = []\n",
    "    frame_count = -1\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        success, frame = cap.read()\n",
    "        if success:\n",
    "            frame_count += 1\n",
    "            annotated_frame = inference_func(frame)\n",
    "            resized_frame = imresize(\n",
    "                cv.cvtColor(annotated_frame, cv.COLOR_BGR2RGB), width=img_width\n",
    "            )\n",
    "            img_list.append(resized_frame)\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    imageio.mimsave(output_gif_path, img_list, fps=fps, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displya Video for Predictions\n",
    "\n",
    "Displays the video along with real-time model predictions in a sample dedicated window. It accepts the path to the video file and the model as inputs. This function offers an immediate, visual understanding of the model's performance on dynamic scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video_for_predictions(\n",
    "    video_path: str,\n",
    "    inference_func: Callable[[np.ndarray], np.ndarray],\n",
    "    window_name: str = \"Prediction Results\",\n",
    "    fps: int = 60,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays a video with predictions by applying an inference function to each frame in real-time.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input video file.\n",
    "        inference_func (Callable[[np.ndarray], np.ndarray]): Function that performs inference on each frame.\n",
    "        window_name (str, optional): Name of the display window. Defaults to \"Prediction Results\".\n",
    "        fps (int, optional): Frames per second for displaying the video. Defaults to 60.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    cv.namedWindow(window_name, cv.WINDOW_NORMAL)\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if success:\n",
    "            annotated_frame = inference_func(frame)\n",
    "            cv.imshow(window_name, annotated_frame)\n",
    "            if cv.waitKey(1000 // fps) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Sample Image\n",
    "\n",
    "This function downloads a image from a specified URL and returns the local path to the downloaded file. This will help us to keep organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sample_image(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads an image from a given URL and saves it to the local file system.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the image to be downloaded.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the downloaded image.\n",
    "\n",
    "    Notes:\n",
    "        If the image already exists in the target directory, it will not be downloaded again.\n",
    "    \"\"\"\n",
    "    img = urlopen(url)\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    img_path = f\"data/{filename}\"\n",
    "    if os.path.isfile(img_path):\n",
    "        print(\"Sample image already downloaded\")\n",
    "    else:\n",
    "        if not os.path.isdir(\"data\"):\n",
    "            os.makedirs(\"data\")\n",
    "        with open(img_path, \"wb\") as f:\n",
    "            f.write(img.read())\n",
    "        print(\"Sample image downloaded successfully\")\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Sample Video\n",
    "\n",
    "This function downloads a video from a specified URL and returns the local path to the downloaded file. It is essential for preparing the data used in demonstrations, ensuring that the same content is accessible for object detection, segmentation, and human pose estimation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sample_video(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a video from YouTube and saves it to the local file system.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the YouTube video to be downloaded.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the downloaded video.\n",
    "\n",
    "    Notes:\n",
    "        If the video already exists in the target directory, it will not be downloaded again.\n",
    "    \"\"\"\n",
    "    youtube_obj = YouTube(url).streams.get_highest_resolution()\n",
    "    video_filepath = os.path.join(\"data\", youtube_obj.default_filename)\n",
    "    if os.path.isfile(video_filepath):\n",
    "        print(\"Sample file already downloaded\")\n",
    "    else:\n",
    "        try:\n",
    "            print(video_filepath, youtube_obj.get_file_path())\n",
    "            youtube_obj.download(output_path=\"data\")\n",
    "            print(\"Sample video downloaded successfully\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error has occurred: {e}\")\n",
    "    return video_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "In this section, we'll download the sample data that will be used to demonstrate how to use the YOLO API on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image already downloaded\n",
      "Sample file already downloaded\n",
      "Sample file already downloaded\n",
      "Sample file already downloaded\n"
     ]
    }
   ],
   "source": [
    "# Image depicting an outdoor environment.\n",
    "bus_img = \"https://ultralytics.com/images/bus.jpg\"\n",
    "# Video of people  walking in a open indoor space.\n",
    "people_walking_video = \"https://youtu.be/ORrrKXGx2SE?si=UZqWGkFnUn7wYdck\"\n",
    "# Video of cars in a traffic road.\n",
    "traffic_cars_video = \"https://youtu.be/MNn9qKG2UFI?si=2U6waPKQJOsTSJYC\"\n",
    "# Video of soccer games moments.\n",
    "soccer_moments_video = \"https://youtu.be/QZGPZR_0qqc?si=K5mBFJAkmf9eDlZZ\"\n",
    "\n",
    "# Download each sample.\n",
    "bus_img_path = download_sample_image(bus_img)\n",
    "people_walking_video_path = download_sample_video(people_walking_video)\n",
    "traffic_cars_video_path = download_sample_video(traffic_cars_video)\n",
    "soccer_moments_video_path = download_sample_video(soccer_moments_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "Our first use case will be the task that made YOLO  models famous.\n",
    "\n",
    "First, we need to load the pre-trained model. As mentioned before, each Ultralytics' YOLO models have variants suited to run on a variety of hardware specifications, and this is where Ultralytics API shines. You can download and load each variant by specifying a suffix in the desired model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO(\"models/yolov8n.pt\")            # <-- \"nano\" model, which has the lowest inference time and hardware requirements, but also the lowest accuracy.\n",
    "# model = YOLO(\"models/yolov8s.pt\")            # <-- \"small\" model.\n",
    "model = YOLO(\"models/yolov8m.pt\")            # -- \"medium\" model, which is a balance between speed and accuracy.\n",
    "# model = YOLO(\"models/yolov8l.pt\")            # <-- \"large\" model.\n",
    "# model = YOLO(\"models/yolov8x.pt\")            # <-- \"extra-large\" model, which has the highest accuracy but also the highest inference time and hardware requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time we run the above cell, it'll download the specified model to the **models** local folder. With the model in place, `model` is ready for inference.\n",
    "\n",
    "To make predictions, is as simple as using `model` as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /mnt/3273eabb-9e14-47b4-8ddd-ddb77dddcd30/workspace/YOLOProjects/data/bus.jpg: 640x480 4 persons, 1 bus, 131.4ms\n",
      "Speed: 2.4ms preprocess, 131.4ms inference, 959.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "results = model(bus_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's predictions are returned as a list of [Results class](https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Results), which has a set of utility attributes and methods that helps retrieving, manipulating and displaying predictions.\n",
    "\n",
    "#### Attributes\n",
    "| Name         | Type                                                                                                     | Description                                                            |\n",
    "| ------------ | -------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |\n",
    "| `orig_image` | ndarray                                                                                                  | Original image as a numpy array.                                       |\n",
    "| `orig_shape` | tuple                                                                                                    | Original image shape in (height, width) format.                        |\n",
    "| `boxes`      | [Boxes](https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Boxes)         | Object containing detection bounding boxes.                            |\n",
    "| `masks`      | [Masks](https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Masks)         | Object containing detection masks.                                     |\n",
    "| `probs`      | [Probs](https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Probs)         | Object containing class probabilities for classification tasks.        |\n",
    "| `keypoints`  | [Keypoints](https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Keypoints) | Object containing detected keypoints for each objects.                 |\n",
    "| `speed`      | dict                                                                                                     | Dictionary of preprocess, inference and postprocess speeds (ms/image). |\n",
    "| `names`      | dict                                                                                                     | Dictionary of class names.                                             |\n",
    "| `path`       | str                                                                                                      | Path to the image file.                                                |\n",
    "\n",
    "#### Methods\n",
    "| Name        | Description                                                                            |\n",
    "| ----------- | -------------------------------------------------------------------------------------- |\n",
    "| `update`    | Updates object attributes with new detection results.                                  |\n",
    "| `cpu`       | Returns a copy of the Results object with all tensors on CPU memory.                   |\n",
    "| `numpy`     | Returns a copy of the Results object with all tensors as numpy arrays.                 |\n",
    "| `cuda`      | Returns a copy of the Results object with all tensors on GPU memory.                   |\n",
    "| `to`        | Returns a copy of the Results object with all tensors on a specified device and dtype. |\n",
    "| `new`       | Returns a new Results object with the same image, path, and names.                     |\n",
    "| `plot`      | Plots detection results on an input image, returning an annotated image.               |\n",
    "| `show`      | Show annotated results on screen.                                                      |\n",
    "| `save`      | Save annotated results to file.                                                        |\n",
    "| `verbose`   | Returns a log string for each task, detailing detections and classifications.          |\n",
    "| `save_txt`  | Saves detection results to a text file.                                                |\n",
    "| `save_crop` | Saves cropped detection images.                                                        |\n",
    "| `tojson`    | Converts detection results to JSON format.                                             |\n",
    "\n",
    "We'll take more about the Results class in our practical projects. For now, I'll focus only in demonstrating the Utralytics API's core functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Predictions\n",
    "\n",
    "The Results class offers some utility functions that helps in quick plotting, visualizing and saving prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imgs/bus_detections.png'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the first set of results.\n",
    "result = results[0]\n",
    "\n",
    "# bus_detections = result.plot()            # <-- Returns the annotated image as a Numpy array.\n",
    "# result.show()                             # <-- Open up a new window displaying the annotated image.\n",
    "result.save(\"imgs/bus_detections.png\")    # <-- Saves the annotated image as a PNG file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the object detector is a set of bounding boxes that enclose the objects in the image, along with class labels and confidence scores for each box.\n",
    "\n",
    "<img src=\"imgs/bus_detections.png\" height=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions on a video stream is also straightforward.\n",
    "\n",
    "**Note:** It's recommended to have a GPU to run the inference on videos and get predictions in an acceptable framerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell displays the video with detections in a dedicated window.\n",
    "\n",
    "display_video_for_predictions(\n",
    "    people_walking_video_path,\n",
    "    lambda img: model(img, verbose=False)[0].plot()     # The verbose=False parameter disables the predictions logs.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a animated GIF from the video detections.\n",
    "\n",
    "create_gif_for_predictions(\n",
    "    people_walking_video_path,\n",
    "    \"imgs/people_detection.gif\",\n",
    "    lambda img: model(img, verbose=False)[0].plot()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/people_detection.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "Image classification is the simplest of the three tasks supported by YOLOv8 models. To download and use the pre-trained model is a similar process of downloading the object detection models, but using the `-cls` at the end of the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO(\"models/yolov8n-cls.pt\")\n",
    "# model = YOLO(\"models/yolov8s-cls.pt\")\n",
    "model = YOLO(\"models/yolov8m-cls.pt\")\n",
    "# model = YOLO(\"models/yolov8l-cls.pt\")\n",
    "# model = YOLO(\"models/yolov8x-cls.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /mnt/3273eabb-9e14-47b4-8ddd-ddb77dddcd30/workspace/YOLOProjects/data/bus.jpg: 224x224 minibus 0.70, police_van 0.20, streetcar 0.03, trolleybus 0.02, amphibian 0.01, 3.6ms\n",
      "Speed: 7.6ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "results = model(bus_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imgs/bus_classification.png'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = results[0]\n",
    "result.save(\"imgs/bus_classification.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the classifier is top-most probable classes present in the image and their confidence scores.\n",
    "\n",
    "<img src=\"imgs/bus_classification.png\" height=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "This example illustrates how YOLOv8 can differentiate and segment multiple vehicles moving in a dynamic environment, highlighting the model's potential in applications such as traffic management and autonomous driving technologies.\n",
    "\n",
    "To use the segmentation models is just a matter of adding the suffix `-seg` to the name of each model we mentioned for the object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO(\"models/yolov8n-seg.pt\")\n",
    "# model = YOLO(\"models/yolov8s-seg.pt\")\n",
    "model = YOLO(\"models/yolov8m-seg.pt\")\n",
    "# model = YOLO(\"models/yolov8l-seg.pt\")\n",
    "# model = YOLO(\"models/yolov8x-seg.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of an instance segmentation model is a set of masks or contours that outline each object in the image, along with class labels and confidence scores for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video_for_predictions(\n",
    "    traffic_cars_video_path,\n",
    "    lambda img: model(img, verbose=False)[0].plot()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gif_for_predictions(\n",
    "    traffic_cars_video_path,\n",
    "    \"imgs/cars_segmentation.gif\",\n",
    "    lambda img: model(img, verbose=False)[0].plot()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/cars_segmentation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Pose Estimation\n",
    "\n",
    "Last but not least, through this example, we demonstrate YOLOv8's ability to accurately estimate human poses in real-time. This scenario is particularly challenging due to the rapid movements and interaction between the soccer players, showcasing its applicability in areas such as sports analysis, animation, and surveillance.\n",
    "\n",
    "Similar t the other models, we select the pose models by adding `-pose` suffix to the models names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO(\"models/yolov8n-pose.pt\")\n",
    "# model = YOLO(\"models/yolov8s-pose.pt\")\n",
    "model = YOLO(\"models/yolov8m-pose.pt\")\n",
    "# model = YOLO(\"models/yolov8l-pose.pt\")\n",
    "# model = YOLO(\"models/yolov8x-pose.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of a pose estimation model is a set of points that represent the keypoints on an object in the image, usually along with the confidence scores for each point. The keypoints can represent various parts of the object such as joints, landmarks, or other distinctive features. The locations of the keypoinbts are represented as a set of 2D `[x, y]` or 3D `[x, y, visible]` coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video_for_predictions(\n",
    "    soccer_moments_video_path,\n",
    "    lambda img: model(img, verbose=False)[0].plot()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gif_for_predictions(\n",
    "    soccer_moments_video_path,\n",
    "    \"imgs/soccer_pose_estimation.gif\",\n",
    "    lambda img: model(img, verbose=False)[0].plot()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/soccer_pose_estimation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
