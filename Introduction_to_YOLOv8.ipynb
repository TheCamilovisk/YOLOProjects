{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Computer Vision is a fields of computer science that focuses on enabling artificial systems to extract information from images and its variants (e.g.: video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, medical scanning devices, etc.). It makes use of algorithmic models that allow a computer to \"teach\" itself the context of visual data, learning the patterns that distinguish an image from another. Computational vision is rapidly gaining popularity for automated AI vision inspection, remote monitoring, and automation.\n",
    "\n",
    "Computer Vision has become the backbone of numerous practical applications that significantly impact our daily lives and companies across industries, from retail to security, healthcare, construction, automotive, manufacturing, logistics, and agriculture.\n",
    "\n",
    "In this scenario, one of the most groundbreaking approaches for Computer Vision is the **You Only Look Once** (YOLO) models family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development History\n",
    "\n",
    "The first version of YOLO(You Look Only Once) was conceived by Joseph Redmon at al. in their 2015 [paper](https://arxiv.org/abs/1506.02640), and represented a groundbreaking approach in Computer Vision, particularly in object *detection tasks.* At the, time the conventional object detection frameworks (e.g.: RCNN) relied on a two-step approach: for a given image, one model is responsible for extraction of regions of objects, and a second model is responsible for classification and refinement of localization of objects. YOLOv1 (how the first version became known) challenged this convention by proposing a single neural network that predictions bounding boxes and class probabilities directly from full images in one evaluation. The approach significantly increased the speed of detection, making real-time object detection feasible.\n",
    "\n",
    "![RCNN pipeline](imgs/rcnn-pipeline.png)\n",
    "*RCNN's multi-stage detection. Credits: RCNN original [paper](https://arxiv.org/abs/1311.2524).*\n",
    "\n",
    "![YOLOv1 pipeline](imgs/yolov1-pipeline.png)\n",
    "*YOLOv1 unified detection. Credits: YOLOv1 original [paper](https://arxiv.org/abs/1506.02640).*\n",
    "\n",
    "Following the initial release, the YOLO architecture underwent several iterations and improvements, leading to version like YOLOv2 ([YOLO9000](https://arxiv.org/abs/1612.08242)), [YOLOv3](https://arxiv.org/abs/1804.02767), and further, each introducing enhancements in speed, accuracy, and the ability to detect smaller objects. [YOLOv4](https://arxiv.org/abs/2004.10934), introduced by Alexey Bochkovskiy, focused on optimizing the speed, accuracy trade-off, making it highly efficient without specialized hardware.\n",
    "\n",
    "The [Ultralytics](https://www.ultralytics.com/) team contributed significantly to the YOLO legacy with their [YOLOv5](https://docs.ultralytics.com/yolov5/) model, which brought improvements in terms of simplicity, speed, and performance. They continued this trend with the development of [YOLOv6](https://arxiv.org/abs/2209.02976) and [YOLOv8](https://docs.ultralytics.com/), which incorporates advanced features and improving upon the accuracy and efficiency of its predecessors.\n",
    "\n",
    "YOLOv8 also supports a full range of vision AI tasks, including detection, segmentation, pose estimation, tracking and classification. This versatility allows users to leverage YOLOv8's capabilities across diverse application and domains.\n",
    "\n",
    "Currently, [YOLO-NAS](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md) and [YOLOv9](https://arxiv.org/abs/2402.13616) were conceived with remarkable improvements in efficiency, accuracy and adaptability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "<img src=\"imgs/yolov8_architecture.jpg\" alt=\"YOLOv8 Architecture\" width=800>*Credits: GitHub user [RangeKing](https://github.com/RangeKing) ([original post](https://github.com/ultralytics/ultralytics/issues/189))*</img>\n",
    "\n",
    "The YOLOv8 architecture was designed to perform object detection tasks with high efficiency and accuracy. While maintaining the core principle of performing object detection in a single pass through the network, YOLOv8 introduces several key improvements and features to enhance performance, incorporating advanced techniques such as:\n",
    "- **Cross-stage Partial Networks (CSPNet):** A backbone designed to reduce redundancy in network layers, reducing it's complexity and improving learning efficiency and model scalability without compromising performance.\n",
    "- **Path Aggregation Network (PANet):** An architecture that enahnces feature extraction and integration ensuring rich semantic information is carried through the network for accurate detection.\n",
    "- **Spatial Pyramid Pooling (SPP):** A pooling strategy that increases the network's robustness to object scale variations, improving detection of objects of various sizes.\n",
    "\n",
    "Additionally, YOLOv8 employs advanced data augmentation techniques and loss functions ([CIoU](https://arxiv.org/abs/1911.08287) and [DFL](https://arxiv.org/abs/2006.04388)) to fine-tune the model's performance further, ensuring it remains robust against a wide variety of images and scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Features\n",
    "\n",
    "YOLOv8 support a versatile range of Computer Vision tasks and pre-trained models for each:\n",
    "- **Image classification**, with models pre-trained in the [ImageNet dataset](https://www.image-net.org/).\n",
    "- **Object detections**, **object segmentation** and **human pose estimation**, with models pre-trained in the [COCO dataset](https://cocodataset.org/#home).\n",
    "\n",
    "Additionally, there are model variatesion for each of these tasks, each variation targeted to run on systems with different hardware specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Requirements\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import cv2 as cv\n",
    "import imageio\n",
    "import numpy as np\n",
    "from pytube import YouTube\n",
    "from ultralytics.models import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Resize Function\n",
    "\n",
    "This function resizes a image to meet a fixed width. This is helpful when creating GIF's, as too big images can result in unwanted big GIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imresize(img: np.ndarray, width: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resizes an image to a specified width while maintaining aspect ratio.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Input image in the form of a NumPy array.\n",
    "        width (int): Desired width of the output image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Resized image as a NumPy array.\n",
    "    \"\"\"\n",
    "    _, old_width, _ = img.shape\n",
    "    factor = width / old_width\n",
    "    return cv.resize(img, None, fx=factor, fy=factor, interpolation=cv.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GIF for Predictions\n",
    "\n",
    "This functions converts a sequence of mode predictions on video data into an animated GIF. It takes as input the path to the video file, the output path for the GIF, the model used for predictions, and optimal parameters for duration and *frames per second* (*pfs*). The function processes the video, applies the model to generate the predictions for each frame, and compiles these frames into a GIF. This utility is particularly useful for demonstrating object detection, segmentation and pose estimation capabilities in a dynamic, easily shareable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif_for_predictions(\n",
    "    video_path: str,\n",
    "    output_gif_path: str,\n",
    "    inference_func: Callable[[np.ndarray], np.ndarray],\n",
    "    img_width: int = 640,\n",
    "    max_frames: int = 300,\n",
    "    fps: int = 30,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a GIF from a video by applying an inference function to each frame.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input video file.\n",
    "        output_gif_path (str): Path to save the output GIF file.\n",
    "        inference_func (Callable[[np.ndarray], np.ndarray]): Function that performs inference on each frame.\n",
    "        img_width (int, optional): Desired width of each frame in the GIF. Defaults to 640.\n",
    "        max_frames (int, optional): Maximum number of frames to process from the video. Defaults to 300.\n",
    "        fps (int, optional): Frames per second for the output GIF. Defaults to 30.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    img_list = []\n",
    "    frame_count = -1\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        success, frame = cap.read()\n",
    "        if success:\n",
    "            frame_count += 1\n",
    "            annotated_frame = inference_func(frame)\n",
    "            resized_frame = imresize(\n",
    "                cv.cvtColor(annotated_frame, cv.COLOR_BGR2RGB), width=img_width\n",
    "            )\n",
    "            img_list.append(resized_frame)\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    imageio.mimsave(output_gif_path, img_list, fps=fps, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displya Video for Predictions\n",
    "\n",
    "Displays the video along with real-time model predictions in a sample dedicated window. It accepts the path to the video file and the model as inputs. This function offers an immediate, visual understanding of the model's performance on dynamic scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video_for_predictions(\n",
    "    video_path: str,\n",
    "    inference_func: Callable[[np.ndarray], np.ndarray],\n",
    "    window_name: str = \"Prediction Results\",\n",
    "    fps: int = 60,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays a video with predictions by applying an inference function to each frame in real-time.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input video file.\n",
    "        inference_func (Callable[[np.ndarray], np.ndarray]): Function that performs inference on each frame.\n",
    "        window_name (str, optional): Name of the display window. Defaults to \"Prediction Results\".\n",
    "        fps (int, optional): Frames per second for displaying the video. Defaults to 60.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    cv.namedWindow(window_name, cv.WINDOW_NORMAL)\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if success:\n",
    "            annotated_frame = inference_func(frame)\n",
    "            cv.imshow(window_name, annotated_frame)\n",
    "            if cv.waitKey(1000 // fps) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Sample Image\n",
    "\n",
    "This function downloads a image from a specified URL and returns the local path to the downloaded file. This will help us to keep organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sample_image(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads an image from a given URL and saves it to the local file system.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the image to be downloaded.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the downloaded image.\n",
    "\n",
    "    Notes:\n",
    "        If the image already exists in the target directory, it will not be downloaded again.\n",
    "    \"\"\"\n",
    "    img = urlopen(url)\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    img_path = f\"data/{filename}\"\n",
    "    if os.path.isfile(img_path):\n",
    "        print(\"Sample image already downloaded\")\n",
    "    else:\n",
    "        if not os.path.isdir(\"data\"):\n",
    "            os.makedirs(\"data\")\n",
    "        with open(img_path, \"wb\") as f:\n",
    "            f.write(img.read())\n",
    "        print(\"Sample image downloaded successfully\")\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Sample Video\n",
    "\n",
    "This function downloads a video from a specified URL and returns the local path to the downloaded file. It is essential for preparing the data used in demonstrations, ensuring that the same content is accessible for object detection, segmentation, and human pose estimation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sample_video(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a video from YouTube and saves it to the local file system.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the YouTube video to be downloaded.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the downloaded video.\n",
    "\n",
    "    Notes:\n",
    "        If the video already exists in the target directory, it will not be downloaded again.\n",
    "    \"\"\"\n",
    "    youtube_obj = YouTube(url).streams.get_highest_resolution()\n",
    "    video_filepath = os.path.join(\"data\", youtube_obj.default_filename)\n",
    "    if os.path.isfile(video_filepath):\n",
    "        print(\"Sample file already downloaded\")\n",
    "    else:\n",
    "        try:\n",
    "            print(video_filepath, youtube_obj.get_file_path())\n",
    "            youtube_obj.download(output_path=\"data\")\n",
    "            print(\"Sample video downloaded successfully\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error has occurred: {e}\")\n",
    "    return video_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "In this section, we'll download the sample data that will be used to demonstrate how to use the YOLO API on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image already downloaded\n",
      "Sample file already downloaded\n",
      "Sample file already downloaded\n",
      "Sample file already downloaded\n"
     ]
    }
   ],
   "source": [
    "# Image depicting an outdoor environment.\n",
    "bus_img = \"https://ultralytics.com/images/bus.jpg\"\n",
    "# Video of people  walking in a open indoor space.\n",
    "people_walking_video = \"https://youtu.be/ORrrKXGx2SE?si=UZqWGkFnUn7wYdck\"\n",
    "# Video of cars in a traffic road.\n",
    "traffic_cars_video = \"https://youtu.be/MNn9qKG2UFI?si=2U6waPKQJOsTSJYC\"\n",
    "# Video of soccer games moments.\n",
    "soccer_moments_video = \"https://youtu.be/aTTOQtSOX3I?si=w1Gvm6hI0qySu5qt\"\n",
    "\n",
    "# Download each sample.\n",
    "bus_img_path = download_sample_image(bus_img)\n",
    "people_walking_video_path = download_sample_video(people_walking_video)\n",
    "traffic_cars_video_path = download_sample_video(traffic_cars_video)\n",
    "soccer_moments_video_path = download_sample_video(soccer_moments_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "Our first use case will be the task that made YOLO  models famous.\n",
    "\n",
    "First, we need to load the pre-trained model. As mentioned before, each Ultralytics' YOLO models have variants suited to run on a variety of hardware specifications, and this is where Ultralytics API shines. You can download and load each variant by specifying a suffix in the desired model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO(\"models/yolov8n.pt\")            <-- \"nano\" model, which has the lowest inference time and hardware requirements, but also the lowest accuracy.\n",
    "# model = YOLO(\"models/yolov8s.pt\")            <-- \"small\" model.\n",
    "# model = YOLO(\"models/yolov8m.pt\")            <-- \"medium\" model, which is a balance between speed and accuracy.\n",
    "# model = YOLO(\"models/yolov8l.pt\")            <-- \"large\" model.\n",
    "model = YOLO(\"models/yolov8x.pt\")            # <-- \"extra-large\" model, which has the highest accuracy but also the highest inference time and hardware requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time we run the above cell, it'll download the specified model to the **models** local folder. With the model in place, `model` has the loaded model ready for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Pose Estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
